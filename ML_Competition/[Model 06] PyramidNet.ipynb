{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "       BatchNorm2d-3           [-1, 16, 32, 32]              32\n",
      "            Conv2d-4           [-1, 18, 32, 32]           2,592\n",
      "       BatchNorm2d-5           [-1, 18, 32, 32]              36\n",
      "              ReLU-6           [-1, 18, 32, 32]               0\n",
      "            Conv2d-7           [-1, 18, 32, 32]           2,916\n",
      "       BatchNorm2d-8           [-1, 18, 32, 32]              36\n",
      "        BasicBlock-9           [-1, 18, 32, 32]               0\n",
      "      BatchNorm2d-10           [-1, 18, 32, 32]              36\n",
      "           Conv2d-11           [-1, 20, 32, 32]           3,240\n",
      "      BatchNorm2d-12           [-1, 20, 32, 32]              40\n",
      "             ReLU-13           [-1, 20, 32, 32]               0\n",
      "           Conv2d-14           [-1, 20, 32, 32]           3,600\n",
      "      BatchNorm2d-15           [-1, 20, 32, 32]              40\n",
      "       BasicBlock-16           [-1, 20, 32, 32]               0\n",
      "      BatchNorm2d-17           [-1, 20, 32, 32]              40\n",
      "           Conv2d-18           [-1, 21, 32, 32]           3,780\n",
      "      BatchNorm2d-19           [-1, 21, 32, 32]              42\n",
      "             ReLU-20           [-1, 21, 32, 32]               0\n",
      "           Conv2d-21           [-1, 21, 32, 32]           3,969\n",
      "      BatchNorm2d-22           [-1, 21, 32, 32]              42\n",
      "       BasicBlock-23           [-1, 21, 32, 32]               0\n",
      "      BatchNorm2d-24           [-1, 21, 32, 32]              42\n",
      "           Conv2d-25           [-1, 23, 32, 32]           4,347\n",
      "      BatchNorm2d-26           [-1, 23, 32, 32]              46\n",
      "             ReLU-27           [-1, 23, 32, 32]               0\n",
      "           Conv2d-28           [-1, 23, 32, 32]           4,761\n",
      "      BatchNorm2d-29           [-1, 23, 32, 32]              46\n",
      "       BasicBlock-30           [-1, 23, 32, 32]               0\n",
      "      BatchNorm2d-31           [-1, 23, 32, 32]              46\n",
      "           Conv2d-32           [-1, 25, 32, 32]           5,175\n",
      "      BatchNorm2d-33           [-1, 25, 32, 32]              50\n",
      "             ReLU-34           [-1, 25, 32, 32]               0\n",
      "           Conv2d-35           [-1, 25, 32, 32]           5,625\n",
      "      BatchNorm2d-36           [-1, 25, 32, 32]              50\n",
      "       BasicBlock-37           [-1, 25, 32, 32]               0\n",
      "      BatchNorm2d-38           [-1, 25, 32, 32]              50\n",
      "           Conv2d-39           [-1, 27, 32, 32]           6,075\n",
      "      BatchNorm2d-40           [-1, 27, 32, 32]              54\n",
      "             ReLU-41           [-1, 27, 32, 32]               0\n",
      "           Conv2d-42           [-1, 27, 32, 32]           6,561\n",
      "      BatchNorm2d-43           [-1, 27, 32, 32]              54\n",
      "       BasicBlock-44           [-1, 27, 32, 32]               0\n",
      "      BatchNorm2d-45           [-1, 27, 32, 32]              54\n",
      "           Conv2d-46           [-1, 28, 32, 32]           6,804\n",
      "      BatchNorm2d-47           [-1, 28, 32, 32]              56\n",
      "             ReLU-48           [-1, 28, 32, 32]               0\n",
      "           Conv2d-49           [-1, 28, 32, 32]           7,056\n",
      "      BatchNorm2d-50           [-1, 28, 32, 32]              56\n",
      "       BasicBlock-51           [-1, 28, 32, 32]               0\n",
      "      BatchNorm2d-52           [-1, 28, 32, 32]              56\n",
      "           Conv2d-53           [-1, 30, 32, 32]           7,560\n",
      "      BatchNorm2d-54           [-1, 30, 32, 32]              60\n",
      "             ReLU-55           [-1, 30, 32, 32]               0\n",
      "           Conv2d-56           [-1, 30, 32, 32]           8,100\n",
      "      BatchNorm2d-57           [-1, 30, 32, 32]              60\n",
      "       BasicBlock-58           [-1, 30, 32, 32]               0\n",
      "      BatchNorm2d-59           [-1, 30, 32, 32]              60\n",
      "           Conv2d-60           [-1, 32, 32, 32]           8,640\n",
      "      BatchNorm2d-61           [-1, 32, 32, 32]              64\n",
      "             ReLU-62           [-1, 32, 32, 32]               0\n",
      "           Conv2d-63           [-1, 32, 32, 32]           9,216\n",
      "      BatchNorm2d-64           [-1, 32, 32, 32]              64\n",
      "       BasicBlock-65           [-1, 32, 32, 32]               0\n",
      "      BatchNorm2d-66           [-1, 32, 32, 32]              64\n",
      "           Conv2d-67           [-1, 34, 16, 16]           9,792\n",
      "      BatchNorm2d-68           [-1, 34, 16, 16]              68\n",
      "             ReLU-69           [-1, 34, 16, 16]               0\n",
      "           Conv2d-70           [-1, 34, 16, 16]          10,404\n",
      "      BatchNorm2d-71           [-1, 34, 16, 16]              68\n",
      "        AvgPool2d-72           [-1, 32, 16, 16]               0\n",
      "       BasicBlock-73           [-1, 34, 16, 16]               0\n",
      "      BatchNorm2d-74           [-1, 34, 16, 16]              68\n",
      "           Conv2d-75           [-1, 36, 16, 16]          11,016\n",
      "      BatchNorm2d-76           [-1, 36, 16, 16]              72\n",
      "             ReLU-77           [-1, 36, 16, 16]               0\n",
      "           Conv2d-78           [-1, 36, 16, 16]          11,664\n",
      "      BatchNorm2d-79           [-1, 36, 16, 16]              72\n",
      "       BasicBlock-80           [-1, 36, 16, 16]               0\n",
      "      BatchNorm2d-81           [-1, 36, 16, 16]              72\n",
      "           Conv2d-82           [-1, 37, 16, 16]          11,988\n",
      "      BatchNorm2d-83           [-1, 37, 16, 16]              74\n",
      "             ReLU-84           [-1, 37, 16, 16]               0\n",
      "           Conv2d-85           [-1, 37, 16, 16]          12,321\n",
      "      BatchNorm2d-86           [-1, 37, 16, 16]              74\n",
      "       BasicBlock-87           [-1, 37, 16, 16]               0\n",
      "      BatchNorm2d-88           [-1, 37, 16, 16]              74\n",
      "           Conv2d-89           [-1, 39, 16, 16]          12,987\n",
      "      BatchNorm2d-90           [-1, 39, 16, 16]              78\n",
      "             ReLU-91           [-1, 39, 16, 16]               0\n",
      "           Conv2d-92           [-1, 39, 16, 16]          13,689\n",
      "      BatchNorm2d-93           [-1, 39, 16, 16]              78\n",
      "       BasicBlock-94           [-1, 39, 16, 16]               0\n",
      "      BatchNorm2d-95           [-1, 39, 16, 16]              78\n",
      "           Conv2d-96           [-1, 41, 16, 16]          14,391\n",
      "      BatchNorm2d-97           [-1, 41, 16, 16]              82\n",
      "             ReLU-98           [-1, 41, 16, 16]               0\n",
      "           Conv2d-99           [-1, 41, 16, 16]          15,129\n",
      "     BatchNorm2d-100           [-1, 41, 16, 16]              82\n",
      "      BasicBlock-101           [-1, 41, 16, 16]               0\n",
      "     BatchNorm2d-102           [-1, 41, 16, 16]              82\n",
      "          Conv2d-103           [-1, 43, 16, 16]          15,867\n",
      "     BatchNorm2d-104           [-1, 43, 16, 16]              86\n",
      "            ReLU-105           [-1, 43, 16, 16]               0\n",
      "          Conv2d-106           [-1, 43, 16, 16]          16,641\n",
      "     BatchNorm2d-107           [-1, 43, 16, 16]              86\n",
      "      BasicBlock-108           [-1, 43, 16, 16]               0\n",
      "     BatchNorm2d-109           [-1, 43, 16, 16]              86\n",
      "          Conv2d-110           [-1, 44, 16, 16]          17,028\n",
      "     BatchNorm2d-111           [-1, 44, 16, 16]              88\n",
      "            ReLU-112           [-1, 44, 16, 16]               0\n",
      "          Conv2d-113           [-1, 44, 16, 16]          17,424\n",
      "     BatchNorm2d-114           [-1, 44, 16, 16]              88\n",
      "      BasicBlock-115           [-1, 44, 16, 16]               0\n",
      "     BatchNorm2d-116           [-1, 44, 16, 16]              88\n",
      "          Conv2d-117           [-1, 46, 16, 16]          18,216\n",
      "     BatchNorm2d-118           [-1, 46, 16, 16]              92\n",
      "            ReLU-119           [-1, 46, 16, 16]               0\n",
      "          Conv2d-120           [-1, 46, 16, 16]          19,044\n",
      "     BatchNorm2d-121           [-1, 46, 16, 16]              92\n",
      "      BasicBlock-122           [-1, 46, 16, 16]               0\n",
      "     BatchNorm2d-123           [-1, 46, 16, 16]              92\n",
      "          Conv2d-124           [-1, 48, 16, 16]          19,872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     BatchNorm2d-125           [-1, 48, 16, 16]              96\n",
      "            ReLU-126           [-1, 48, 16, 16]               0\n",
      "          Conv2d-127           [-1, 48, 16, 16]          20,736\n",
      "     BatchNorm2d-128           [-1, 48, 16, 16]              96\n",
      "      BasicBlock-129           [-1, 48, 16, 16]               0\n",
      "     BatchNorm2d-130           [-1, 48, 16, 16]              96\n",
      "          Conv2d-131             [-1, 50, 8, 8]          21,600\n",
      "     BatchNorm2d-132             [-1, 50, 8, 8]             100\n",
      "            ReLU-133             [-1, 50, 8, 8]               0\n",
      "          Conv2d-134             [-1, 50, 8, 8]          22,500\n",
      "     BatchNorm2d-135             [-1, 50, 8, 8]             100\n",
      "       AvgPool2d-136             [-1, 48, 8, 8]               0\n",
      "      BasicBlock-137             [-1, 50, 8, 8]               0\n",
      "     BatchNorm2d-138             [-1, 50, 8, 8]             100\n",
      "          Conv2d-139             [-1, 52, 8, 8]          23,400\n",
      "     BatchNorm2d-140             [-1, 52, 8, 8]             104\n",
      "            ReLU-141             [-1, 52, 8, 8]               0\n",
      "          Conv2d-142             [-1, 52, 8, 8]          24,336\n",
      "     BatchNorm2d-143             [-1, 52, 8, 8]             104\n",
      "      BasicBlock-144             [-1, 52, 8, 8]               0\n",
      "     BatchNorm2d-145             [-1, 52, 8, 8]             104\n",
      "          Conv2d-146             [-1, 53, 8, 8]          24,804\n",
      "     BatchNorm2d-147             [-1, 53, 8, 8]             106\n",
      "            ReLU-148             [-1, 53, 8, 8]               0\n",
      "          Conv2d-149             [-1, 53, 8, 8]          25,281\n",
      "     BatchNorm2d-150             [-1, 53, 8, 8]             106\n",
      "      BasicBlock-151             [-1, 53, 8, 8]               0\n",
      "     BatchNorm2d-152             [-1, 53, 8, 8]             106\n",
      "          Conv2d-153             [-1, 55, 8, 8]          26,235\n",
      "     BatchNorm2d-154             [-1, 55, 8, 8]             110\n",
      "            ReLU-155             [-1, 55, 8, 8]               0\n",
      "          Conv2d-156             [-1, 55, 8, 8]          27,225\n",
      "     BatchNorm2d-157             [-1, 55, 8, 8]             110\n",
      "      BasicBlock-158             [-1, 55, 8, 8]               0\n",
      "     BatchNorm2d-159             [-1, 55, 8, 8]             110\n",
      "          Conv2d-160             [-1, 57, 8, 8]          28,215\n",
      "     BatchNorm2d-161             [-1, 57, 8, 8]             114\n",
      "            ReLU-162             [-1, 57, 8, 8]               0\n",
      "          Conv2d-163             [-1, 57, 8, 8]          29,241\n",
      "     BatchNorm2d-164             [-1, 57, 8, 8]             114\n",
      "      BasicBlock-165             [-1, 57, 8, 8]               0\n",
      "     BatchNorm2d-166             [-1, 57, 8, 8]             114\n",
      "          Conv2d-167             [-1, 59, 8, 8]          30,267\n",
      "     BatchNorm2d-168             [-1, 59, 8, 8]             118\n",
      "            ReLU-169             [-1, 59, 8, 8]               0\n",
      "          Conv2d-170             [-1, 59, 8, 8]          31,329\n",
      "     BatchNorm2d-171             [-1, 59, 8, 8]             118\n",
      "      BasicBlock-172             [-1, 59, 8, 8]               0\n",
      "     BatchNorm2d-173             [-1, 59, 8, 8]             118\n",
      "          Conv2d-174             [-1, 60, 8, 8]          31,860\n",
      "     BatchNorm2d-175             [-1, 60, 8, 8]             120\n",
      "            ReLU-176             [-1, 60, 8, 8]               0\n",
      "          Conv2d-177             [-1, 60, 8, 8]          32,400\n",
      "     BatchNorm2d-178             [-1, 60, 8, 8]             120\n",
      "      BasicBlock-179             [-1, 60, 8, 8]               0\n",
      "     BatchNorm2d-180             [-1, 60, 8, 8]             120\n",
      "          Conv2d-181             [-1, 62, 8, 8]          33,480\n",
      "     BatchNorm2d-182             [-1, 62, 8, 8]             124\n",
      "            ReLU-183             [-1, 62, 8, 8]               0\n",
      "          Conv2d-184             [-1, 62, 8, 8]          34,596\n",
      "     BatchNorm2d-185             [-1, 62, 8, 8]             124\n",
      "      BasicBlock-186             [-1, 62, 8, 8]               0\n",
      "     BatchNorm2d-187             [-1, 62, 8, 8]             124\n",
      "          Conv2d-188             [-1, 64, 8, 8]          35,712\n",
      "     BatchNorm2d-189             [-1, 64, 8, 8]             128\n",
      "            ReLU-190             [-1, 64, 8, 8]               0\n",
      "          Conv2d-191             [-1, 64, 8, 8]          36,864\n",
      "     BatchNorm2d-192             [-1, 64, 8, 8]             128\n",
      "      BasicBlock-193             [-1, 64, 8, 8]               0\n",
      "     BatchNorm2d-194             [-1, 64, 8, 8]             128\n",
      "            ReLU-195             [-1, 64, 8, 8]               0\n",
      "       AvgPool2d-196             [-1, 64, 2, 2]               0\n",
      "================================================================\n",
      "Total params: 894,691\n",
      "Trainable params: 894,691\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.53\n",
      "Params size (MB): 3.41\n",
      "Estimated Total Size (MB): 22.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PyramidNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "#from math import round\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    outchannel_ratio = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)        \n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "       \n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = torch.autograd.Variable(torch.FloatTensor(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1]).fill_(0)) \n",
    "            out += torch.cat((shortcut, padding), 1) # 원래 cuda.FloatTensor\n",
    "        else:\n",
    "            out += shortcut \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    outchannel_ratio = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, (planes*1), kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d((planes*1))\n",
    "        self.conv3 = nn.Conv2d((planes*1), planes * Bottleneck.outchannel_ratio, kernel_size=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(planes * Bottleneck.outchannel_ratio)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.bn1(x)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    " \n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out = self.bn4(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "            featuremap_size = shortcut.size()[2:4]\n",
    "        else:\n",
    "            shortcut = x\n",
    "            featuremap_size = out.size()[2:4]\n",
    "\n",
    "        batch_size = out.size()[0]\n",
    "        residual_channel = out.size()[1]\n",
    "        shortcut_channel = shortcut.size()[1]\n",
    "\n",
    "        if residual_channel != shortcut_channel:\n",
    "            padding = torch.autograd.Variable(torch.cuda.FloatTensor(batch_size, residual_channel - shortcut_channel, featuremap_size[0], featuremap_size[1]).fill_(0)) \n",
    "            out += torch.cat((shortcut, padding), 1)\n",
    "        else:\n",
    "            out += shortcut \n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PyramidNet(nn.Module):\n",
    "        \n",
    "    def __init__(self, dataset, depth, alpha, num_classes, bottleneck=False):\n",
    "        super(PyramidNet, self).__init__()   \t\n",
    "        self.dataset = dataset\n",
    "        if self.dataset.startswith('cifar'):\n",
    "            self.inplanes = 16\n",
    "            if bottleneck == True:\n",
    "                n = int((depth - 2) / 9)\n",
    "                block = Bottleneck\n",
    "            else:\n",
    "                n = int((depth - 2) / 6)\n",
    "                block = BasicBlock\n",
    "\n",
    "            self.addrate = alpha / (3*n*1.0)\n",
    "\n",
    "            self.input_featuremap_dim = self.inplanes\n",
    "            self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
    "\n",
    "            self.featuremap_dim = self.input_featuremap_dim \n",
    "            self.layer1 = self.pyramidal_make_layer(block, n)\n",
    "            self.layer2 = self.pyramidal_make_layer(block, n, stride=2)\n",
    "            self.layer3 = self.pyramidal_make_layer(block, n, stride=2)\n",
    "\n",
    "            self.final_featuremap_dim = self.input_featuremap_dim\n",
    "            self.bn_final= nn.BatchNorm2d(self.final_featuremap_dim)\n",
    "            self.relu_final = nn.ReLU(inplace=True)\n",
    "            self.avgpool = nn.AvgPool2d(3)\n",
    "            self.fc = nn.Linear(self.final_featuremap_dim, num_classes)\n",
    "\n",
    "        elif dataset == 'imagenet':\n",
    "            blocks ={18: BasicBlock, 34: BasicBlock, 50: Bottleneck, 101: Bottleneck, 152: Bottleneck, 200: Bottleneck}\n",
    "            layers ={18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3], 200: [3, 24, 36, 3]}\n",
    "\n",
    "            if layers.get(depth) is None:\n",
    "                if bottleneck == True:\n",
    "                    blocks[depth] = Bottleneck\n",
    "                    temp_cfg = int((depth-2)/12)\n",
    "                else:\n",
    "                    blocks[depth] = BasicBlock\n",
    "                    temp_cfg = int((depth-2)/8)\n",
    "\n",
    "                layers[depth]= [temp_cfg, temp_cfg, temp_cfg, temp_cfg]\n",
    "                print('=> the layer configuration for each stage is set to', layers[depth])\n",
    "\n",
    "            self.inplanes = 64            \n",
    "            self.addrate = alpha / (sum(layers[depth])*1.0)\n",
    "\n",
    "            self.input_featuremap_dim = self.inplanes\n",
    "            self.conv1 = nn.Conv2d(3, self.input_featuremap_dim, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(self.input_featuremap_dim)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "            self.featuremap_dim = self.input_featuremap_dim \n",
    "            self.layer1 = self.pyramidal_make_layer(blocks[depth], layers[depth][0])\n",
    "            self.layer2 = self.pyramidal_make_layer(blocks[depth], layers[depth][1], stride=2)\n",
    "            self.layer3 = self.pyramidal_make_layer(blocks[depth], layers[depth][2], stride=2)\n",
    "            self.layer4 = self.pyramidal_make_layer(blocks[depth], layers[depth][3], stride=2)\n",
    "\n",
    "            self.final_featuremap_dim = self.input_featuremap_dim\n",
    "            self.bn_final= nn.BatchNorm2d(self.final_featuremap_dim)\n",
    "            self.relu_final = nn.ReLU(inplace=True)\n",
    "            self.avgpool = nn.AvgPool2d(7) \n",
    "            self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def pyramidal_make_layer(self, block, block_depth, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.AvgPool2d((2,2), stride = (2, 2), ceil_mode=True)\n",
    "\n",
    "        layers = []\n",
    "        self.featuremap_dim = self.featuremap_dim + self.addrate\n",
    "        layers.append(block(self.input_featuremap_dim, int(round(self.featuremap_dim)), stride, downsample))\n",
    "        for i in range(1, block_depth):\n",
    "            temp_featuremap_dim = self.featuremap_dim + self.addrate\n",
    "            layers.append(block(int(round(self.featuremap_dim)) * block.outchannel_ratio, int(round(temp_featuremap_dim)), 1))\n",
    "            self.featuremap_dim  = temp_featuremap_dim\n",
    "        self.input_featuremap_dim = int(round(self.featuremap_dim)) * block.outchannel_ratio\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.dataset == 'cifar10' or self.dataset == 'cifar100':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            \n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "\n",
    "            x = self.bn_final(x)\n",
    "            x = self.relu_final(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            #x = self.fc(x)\n",
    "\n",
    "        elif self.dataset == 'imagenet':\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "\n",
    "            x = self.bn_final(x)\n",
    "            x = self.relu_final(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "def Model():\n",
    "    r\"\"\"Return your custom model\n",
    "    \"\"\"\n",
    "    return PyramidNet('cifar10', 60, 48, 10)\n",
    "\n",
    "from torchsummary import summary\n",
    "model = Model()\n",
    "summary(model, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
