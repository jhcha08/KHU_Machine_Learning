{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20201110.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1nk9igqtVlKE0mE1BBzO4dfpzrkkQD-bF","authorship_tag":"ABX9TyMw3DVN9onkLX5gkk2obg2N"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vgD9aWohhPJy"},"source":["Lecture 06."]},{"cell_type":"code","metadata":{"id":"4lk3cYx9r2Rv"},"source":["from torch import tensor\n","from torch import nn\n","from torch import sigmoid\n","import torch.optim as optim\n","\n","x_data = tensor([[1.0],[2.0],[3.0],[4.0]])\n","y_data = tensor([[0.],[0.],[1.0],[1.0]])\n","\n","class Model(nn.Module):\n","  # 생성자와 forward를 오버라이딩한다 (= 추가한다)\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.linear = nn.Linear(1,1) # 1개의 노드로 입력을 받아 1개의 출력을 낸다, 레이어는 하나\n","\n","  def forward(self, x): # 실제 모델이 동작하는 부분\n","    y_pred = sigmoid(self.linear(x))\n","    return y_pred\n","\n","model = Model()\n","\n","criterion = nn.BCELoss(reduction='mean') # 1/n을 곱해준다는 것. 설정 안하면 그냥 sum만 함\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","for epoch in range(1000):\n","  y_pred = model(x_data)\n","\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch {epoch+1}/1000 | Loss: {loss.item(): .4f}')\n","\n","  optimizer.zero_grad() # gradient 0으로 초기화\n","  loss.backward() # 각 파라미터에 대한 gradient를 구함\n","  optimizer.step() # 가중치 업데이트\n","\n","# After training\n","print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n","hour_var = model(tensor([[1.0]]))\n","print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n","hour_var = model(tensor([[7.0]]))\n","print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Up-2uksahT6K"},"source":["Lecture 07."]},{"cell_type":"code","metadata":{"id":"K1MGqmuur5hj"},"source":["from torch import nn, optim, from_numpy\n","import numpy as np\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","xy = np.loadtxt('/content/drive/My Drive/머신러닝 수업 실습/diabetes.csv.gz', delimiter=',',dtype=np.float32)\n","x_data = from_numpy(xy[:, 0:-1])\n","y_data = from_numpy(xy[:, [-1]])\n","print(f\"XW's shape: {x_data.shape} | YW's shape: {y_data.shape}\")\n","\n","class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.l1 = nn.Linear(8,6)\n","    self.l2 = nn.Linear(6,4) # 이전 output 노드와 앞의 input 노드의 수가 같아야 함\n","    self.l3 = nn.Linear(4,1)\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    out1 = self.sigmoid(self.l1(x))\n","    out2 = self.sigmoid(self.l2(out1))\n","    y_pred = self.sigmoid(self.l3(out2))\n","    return y_pred\n","\n","model = Model()\n","\n","criterion = nn.BCELoss(size_average = True)\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","for epoch in range(1000):\n","  y_pred = model(x_data)\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch+1}/1000 | Loss: {loss.item(): .4f}')\n","\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eaCBN7caVjva"},"source":["Exercise 7-1 (More than 10 layers, another activation function, minimizing loss)"]},{"cell_type":"code","metadata":{"id":"qTgcI2fKr9NU"},"source":["from torch import nn, optim, from_numpy\n","import numpy as np\n","from google.colab import drive\n","import torch.nn.functional as F\n","\n","drive.mount('/content/drive')\n","\n","xy = np.loadtxt('/content/drive/My Drive/머신러닝 수업 실습/diabetes.csv.gz', delimiter=',',dtype=np.float32)\n","\n","x_data = from_numpy(xy[:, 0:-1])\n","y_data = from_numpy(xy[:, [-1]])\n","\n","print(f\"XW's shape: {x_data.shape} | YW's shape: {y_data.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apT56ezUr_Do"},"source":["# 레이어를 10층 이상으로 늘림\n","class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.l1 = nn.Linear(8,7)\n","    self.l2 = nn.Linear(7,6) # 이전 output 노드와 앞의 input 노드의 수가 같아야 함\n","    self.l3 = nn.Linear(6,6)\n","    self.l4 = nn.Linear(6,6)\n","    self.l5 = nn.Linear(6,5)\n","    self.l6 = nn.Linear(5,5)\n","    self.l7 = nn.Linear(5,5)\n","    self.l8 = nn.Linear(5,4)\n","    self.l9 = nn.Linear(4,4)\n","    self.l10 = nn.Linear(4,4)\n","    self.l11 = nn.Linear(4,1)\n","\n","# 시그모이드 대신 relu 활성화 함수를 사용함, 마지막엔 분류를 위해 시그모이드\n","  def forward(self, x):\n","    x = F.relu(self.l1(x))\n","    x = F.relu(self.l2(x))\n","    x = F.relu(self.l3(x))\n","    x = F.relu(self.l4(x))\n","    x = F.relu(self.l5(x))\n","    x = F.relu(self.l6(x))\n","    x = F.relu(self.l7(x))\n","    x = F.relu(self.l8(x))\n","    x = F.relu(self.l9(x))\n","    x = F.relu(self.l10(x))\n","    y_pred =  F.sigmoid(self.l11(x))\n","    return y_pred\n","\n","model = Model()\n","\n","# 옵티마이저를 SGD 대신 Adam을 사용했더니 loss가 많이 줄음\n","criterion = nn.BCELoss(size_average = True)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","for epoch in range(1000):\n","  y_pred = model(x_data)\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch+1}/1000 | Loss: {loss.item(): .4f}')\n","\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KtduD4YjYdjy"},"source":["Exercise 7-1 (Try other datasets, iris)"]},{"cell_type":"code","metadata":{"id":"HqGy04zWsBAx"},"source":["from torch import nn, optim, from_numpy\n","import pandas as pd\n","import numpy as np\n","from google.colab import drive\n","import torch.nn.functional as F\n","\n","drive.mount('/content/drive')\n","\n","xy = pd.read_csv('/content/drive/My Drive/머신러닝 수업 실습/Iris.csv', delimiter=',',dtype=np.float32)\n","xy = np.array(xy)\n","\n","x_data = from_numpy(xy[:, 0:-1])\n","y_data = from_numpy(xy[:, [-1]])\n","\n","print(f\"XW's shape: {x_data.shape} | YW's shape: {y_data.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6oO57d4sC-A"},"source":["class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.l1 = nn.Linear(4,4)\n","    self.l2 = nn.Linear(4,4)\n","    self.l3 = nn.Linear(4,4)\n","    self.l4 = nn.Linear(4,3)\n","\n","  def forward(self, x):\n","    x = F.relu(self.l1(x))\n","    x = F.relu(self.l2(x))\n","    x = F.relu(self.l3(x))\n","    y_pred =  F.relu(self.l4(x))\n","    return y_pred\n","\n","model = Model()\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","y_pred = model(x_data)\n","\n","for epoch in range(1000):\n","  y_pred = model(x_data)\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch+1}/1000 | Loss: {loss.item(): .4f}')\n","\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()"],"execution_count":null,"outputs":[]}]}
