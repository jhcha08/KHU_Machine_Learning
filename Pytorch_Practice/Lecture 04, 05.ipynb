{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20201105.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPbI3PLTKggCOTW0qOVTadO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8MncOsY97q80"},"source":["Lecture 04."]},{"cell_type":"code","metadata":{"id":"AwEjOlRMatd3","executionInfo":{"status":"ok","timestamp":1604726096534,"user_tz":-540,"elapsed":1162,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"583cd000-e984-42f2-a571-96b4ce73a628","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Auto Gradient\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x_data = [1.0,2.0,3.0]\n","y_data = [2.0,4.0,6.0]\n","\n","w = torch.tensor([1.0], requires_grad=True)\n","\n","# our model forward pass\n","def forward(x,b):\n","  return x * w\n","\n","# loss function\n","def loss(y_pred, y_val):\n","    return (y_pred - y_val) ** 2\n","\n","# before training\n","print('predict (before training)', 4, forward(4,b).item())\n","\n","for epoch in range(10):\n","  for x_val, y_val in zip(x_data, y_data):\n","    y_pred = forward(x_val,b) # 1. Forward pass\n","    l = loss(y_pred, y_val) # 2. Compute loss\n","    l.backward() # 3. Back Propagation to update weights - 이 메소드가 loss의 미분값을 다 계산\n","    print('\\tgrad: ', x_val, y_val, w.grad.item()) # - w에 대한 loss의 미분값이 w.grad.item()에 저장됨\n","    w.data = w.data - 0.01 * w.grad.item()\n","\n","    # Manually zero the gradients after updating weights\n","    w.grad.data.zero_()\n","\n","  print(f'Epoch: {epoch} | Loss: {l.item()}')\n","\n","# After training\n","print('Prediction (after training)', '4hours', forward(4,b).item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict (before training) 4 6.0\n","\tgrad:  1.0 2.0 2.0\n","\tgrad:  2.0 4.0 -0.15999984741210938\n","\tgrad:  3.0 6.0 -6.331197738647461\n","Epoch: 0 | Loss: 1.1134462356567383\n","\tgrad:  1.0 2.0 2.0898237228393555\n","\tgrad:  2.0 4.0 0.1921100616455078\n","\tgrad:  3.0 6.0 -5.60233211517334\n","Epoch: 1 | Loss: 0.87183678150177\n","\tgrad:  1.0 2.0 2.1562318801879883\n","\tgrad:  2.0 4.0 0.45243072509765625\n","\tgrad:  3.0 6.0 -5.063472747802734\n","Epoch: 2 | Loss: 0.712187647819519\n","\tgrad:  1.0 2.0 2.2053279876708984\n","\tgrad:  2.0 4.0 0.6448860168457031\n","\tgrad:  3.0 6.0 -4.665083885192871\n","Epoch: 3 | Loss: 0.60452800989151\n","\tgrad:  1.0 2.0 2.2416257858276367\n","\tgrad:  2.0 4.0 0.7871742248535156\n","\tgrad:  3.0 6.0 -4.370550155639648\n","Epoch: 4 | Loss: 0.5306029915809631\n","\tgrad:  1.0 2.0 2.268460750579834\n","\tgrad:  2.0 4.0 0.8923664093017578\n","\tgrad:  3.0 6.0 -4.152800559997559\n","Epoch: 5 | Loss: 0.4790486693382263\n","\tgrad:  1.0 2.0 2.2883005142211914\n","\tgrad:  2.0 4.0 0.9701366424560547\n","\tgrad:  3.0 6.0 -3.991816520690918\n","Epoch: 6 | Loss: 0.44262775778770447\n","\tgrad:  1.0 2.0 2.3029680252075195\n","\tgrad:  2.0 4.0 1.0276336669921875\n","\tgrad:  3.0 6.0 -3.872797966003418\n","Epoch: 7 | Loss: 0.41662678122520447\n","\tgrad:  1.0 2.0 2.313811779022217\n","\tgrad:  2.0 4.0 1.0701408386230469\n","\tgrad:  3.0 6.0 -3.7848072052001953\n","Epoch: 8 | Loss: 0.39791014790534973\n","\tgrad:  1.0 2.0 2.321828842163086\n","\tgrad:  2.0 4.0 1.1015701293945312\n","\tgrad:  3.0 6.0 -3.71975040435791\n","Epoch: 9 | Loss: 0.38434842228889465\n","Prediction (after training) 4hours 6.655511856079102\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"392Pl5W1metG","executionInfo":{"status":"ok","timestamp":1604724309438,"user_tz":-540,"elapsed":1671,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"e6d91cac-4ae9-47f9-bd02-27b9315bc988","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Manual Gradient\n","\n","# Training Data\n","x_data = [1.0, 2.0, 3.0]\n","y_data = [2.0, 4.0, 6.0]\n","\n","w = 1.0  # a random guess: random value\n","\n","# our model forward pass\n","def forward(x):\n","    return x * w\n","\n","# Loss function\n","def loss(x, y):\n","    y_pred = forward(x)\n","    return (y_pred - y) * (y_pred - y)\n","\n","# compute gradient\n","def gradient(x, y):  # d_loss/d_w\n","    return 2 * x * (x * w - y)\n","\n","# before training\n","print('predict (before training)', 4, forward(4))\n","\n","# Training loop\n","for epoch in range(10):\n","  for x_val, y_val in zip(x_data, y_data):\n","    grad = gradient(x_val, y_val)\n","    w = w - 0.01*grad\n","    print('\\tgrad: ', x_val, y_val, grad)\n","    l = loss(x_val, y_val)\n","  print('progress: ', epoch, l)\n","\n","# After training\n","print('Prediction (after training)', '4hours', forward(4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict (before training) 4 4.0\n","\tgrad:  1.0 2.0 -2.0\n","\tgrad:  2.0 4.0 -7.84\n","\tgrad:  3.0 6.0 -16.2288\n","progress:  0 4.919240100095999\n","\tgrad:  1.0 2.0 -1.478624\n","\tgrad:  2.0 4.0 -5.796206079999999\n","\tgrad:  3.0 6.0 -11.998146585599997\n","progress:  1 2.688769240265834\n","\tgrad:  1.0 2.0 -1.093164466688\n","\tgrad:  2.0 4.0 -4.285204709416961\n","\tgrad:  3.0 6.0 -8.87037374849311\n","progress:  2 1.4696334962911515\n","\tgrad:  1.0 2.0 -0.8081896081960389\n","\tgrad:  2.0 4.0 -3.1681032641284723\n","\tgrad:  3.0 6.0 -6.557973756745939\n","progress:  3 0.8032755585999681\n","\tgrad:  1.0 2.0 -0.59750427561463\n","\tgrad:  2.0 4.0 -2.3422167604093502\n","\tgrad:  3.0 6.0 -4.848388694047353\n","progress:  4 0.43905614881022015\n","\tgrad:  1.0 2.0 -0.44174208101320334\n","\tgrad:  2.0 4.0 -1.7316289575717576\n","\tgrad:  3.0 6.0 -3.584471942173538\n","progress:  5 0.2399802903801062\n","\tgrad:  1.0 2.0 -0.3265852213980338\n","\tgrad:  2.0 4.0 -1.2802140678802925\n","\tgrad:  3.0 6.0 -2.650043120512205\n","progress:  6 0.1311689630744999\n","\tgrad:  1.0 2.0 -0.241448373202223\n","\tgrad:  2.0 4.0 -0.946477622952715\n","\tgrad:  3.0 6.0 -1.9592086795121197\n","progress:  7 0.07169462478267678\n","\tgrad:  1.0 2.0 -0.17850567968888198\n","\tgrad:  2.0 4.0 -0.6997422643804168\n","\tgrad:  3.0 6.0 -1.4484664872674653\n","progress:  8 0.03918700813247573\n","\tgrad:  1.0 2.0 -0.13197139106214673\n","\tgrad:  2.0 4.0 -0.5173278529636143\n","\tgrad:  3.0 6.0 -1.0708686556346834\n","progress:  9 0.021418922423117836\n","Prediction (after training) 4 7.804863933862125\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZEzbXLUvLPy5"},"source":["Exercise 4-1, 4-2"]},{"cell_type":"code","metadata":{"id":"FlIVDhodrA-O","executionInfo":{"status":"ok","timestamp":1604727209743,"user_tz":-540,"elapsed":1153,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"28061c9d-f609-4d57-8518-2100f65f4537","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x_data = [1.0,2.0,3.0]\n","y_data = [2.0,4.0,6.0]\n","\n","w = torch.tensor([1.0], requires_grad=True)\n","b = torch.tensor([2.0], requires_grad=True)\n","\n","# our model forward pass\n","def forward(x):\n","  return x * w + b\n","\n","# loss function\n","def loss(y_pred, y_val):\n","    return (y_pred - y_val) ** 2\n","\n","# before training\n","print('predict (before training)', 4, forward(4).item())\n","\n","for epoch in range(10):\n","  for x_val, y_val in zip(x_data, y_data):\n","    y_pred = forward(x_val) # 1. Forward pass\n","    l = loss(y_pred, y_val) # 2. Compute loss\n","    l.backward() # 3. Back Propagation to update weights - 이 메소드가 loss의 미분값을 다 계산\n","    print('\\tgrad: ', x_val, y_val, w.grad.item()) # - w에 대한 loss의 미분값이 w.grad.item()에 저장됨\n","    w.data = w.data - 0.01 * w.grad.item()\n","\n","    # Manually zero the gradients after updating weights\n","    w.grad.data.zero_()\n","\n","  print(f'Epoch: {epoch} | Loss: {l.item()}')\n","\n","# After training\n","print('Prediction (after training)', '4hours', forward(4).item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict (before training) 4 6.0\n","\tgrad:  1.0 2.0 2.0\n","\tgrad:  2.0 4.0 -0.15999984741210938\n","\tgrad:  3.0 6.0 -6.331197738647461\n","Epoch: 0 | Loss: 1.1134462356567383\n","\tgrad:  1.0 2.0 2.0898237228393555\n","\tgrad:  2.0 4.0 0.1921100616455078\n","\tgrad:  3.0 6.0 -5.60233211517334\n","Epoch: 1 | Loss: 0.87183678150177\n","\tgrad:  1.0 2.0 2.1562318801879883\n","\tgrad:  2.0 4.0 0.45243072509765625\n","\tgrad:  3.0 6.0 -5.063472747802734\n","Epoch: 2 | Loss: 0.712187647819519\n","\tgrad:  1.0 2.0 2.2053279876708984\n","\tgrad:  2.0 4.0 0.6448860168457031\n","\tgrad:  3.0 6.0 -4.665083885192871\n","Epoch: 3 | Loss: 0.60452800989151\n","\tgrad:  1.0 2.0 2.2416257858276367\n","\tgrad:  2.0 4.0 0.7871742248535156\n","\tgrad:  3.0 6.0 -4.370550155639648\n","Epoch: 4 | Loss: 0.5306029915809631\n","\tgrad:  1.0 2.0 2.268460750579834\n","\tgrad:  2.0 4.0 0.8923664093017578\n","\tgrad:  3.0 6.0 -4.152800559997559\n","Epoch: 5 | Loss: 0.4790486693382263\n","\tgrad:  1.0 2.0 2.2883005142211914\n","\tgrad:  2.0 4.0 0.9701366424560547\n","\tgrad:  3.0 6.0 -3.991816520690918\n","Epoch: 6 | Loss: 0.44262775778770447\n","\tgrad:  1.0 2.0 2.3029680252075195\n","\tgrad:  2.0 4.0 1.0276336669921875\n","\tgrad:  3.0 6.0 -3.872797966003418\n","Epoch: 7 | Loss: 0.41662678122520447\n","\tgrad:  1.0 2.0 2.313811779022217\n","\tgrad:  2.0 4.0 1.0701408386230469\n","\tgrad:  3.0 6.0 -3.7848072052001953\n","Epoch: 8 | Loss: 0.39791014790534973\n","\tgrad:  1.0 2.0 2.321828842163086\n","\tgrad:  2.0 4.0 1.1015701293945312\n","\tgrad:  3.0 6.0 -3.71975040435791\n","Epoch: 9 | Loss: 0.38434842228889465\n","Prediction (after training) 4hours 6.655511856079102\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xkJcAsm1U_HI"},"source":["Exercise 4-3"]},{"cell_type":"code","metadata":{"id":"M60W-bDQU-sk","executionInfo":{"status":"ok","timestamp":1604729887929,"user_tz":-540,"elapsed":1196,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"856903d4-48ea-42be-c359-3f0238fbf9a1","colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","\n","x = np.array([2.0])\n","w = np.array([1.0])\n","y = np.array([4.0])\n","\n","y_hat = np.dot(x,w)\n","s = y_hat-y\n","loss = s**2\n","\n","grad_for_s = 2*s\n","grad_for_y_hat = grad_for_s * 1\n","grad_for_w = grad_for_y_hat * x\n","\n","print(grad_for_w)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[-8.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gloJebDBLbFv"},"source":["Exercise 4-5"]},{"cell_type":"code","metadata":{"id":"WbRkGoxuLVa3","executionInfo":{"status":"ok","timestamp":1604729031007,"user_tz":-540,"elapsed":1309,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"d0dcea07-771b-449c-a8e4-294b6095f9a7","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x_data = [1.0,2.0,3.0]\n","y_data = [2.0,4.0,6.0]\n","\n","w1 = torch.tensor([1.0], requires_grad=True) # w1 = 1이라고 가정\n","w2 = torch.tensor([1.0], requires_grad=True) # w2 = 1이라고 가정\n","b = torch.tensor([2.0], requires_grad=True) # b = 2라고 가정\n","\n","# our model forward pass\n","def forward(x):\n","  return (x**2) * w2 + x * w1 + b\n","\n","# loss function\n","def loss(y_pred, y_val):\n","    return (y_pred - y_val) ** 2\n","\n","# before training\n","print('predict (before training)', 4, forward(4).item())\n","\n","for epoch in range(10):\n","  for x_val, y_val in zip(x_data, y_data):\n","    y_pred = forward(x_val) # 1. Forward pass\n","    l = loss(y_pred, y_val) # 2. Compute loss\n","    l.backward() # 3. Back Propagation to update weights - 이 메소드가 loss의 미분값을 다 계산\n","    print('\\tgrad1 & grad2: ', x_val, y_val, w1.grad.item(), w2.grad.item())\n","    w1.data = w1.data - 0.01 * w1.grad.item()\n","    w2.data = w2.data - 0.01 * w2.grad.item()\n","\n","    # Manually zero the gradients after updating weights\n","    w1.grad.data.zero_()\n","    w2.grad.data.zero_()\n","\n","  print(f'Epoch: {epoch} | Loss: {l.item()}')\n","\n","# After training\n","print('Prediction (after training)', '4hours', forward(4).item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predict (before training) 4 22.0\n","\tgrad1 & grad2:  1.0 2.0 4.0 4.0\n","\tgrad1 & grad2:  2.0 4.0 15.039999008178711 30.079998016357422\n","\tgrad1 & grad2:  3.0 6.0 26.169605255126953 78.50881958007812\n","Epoch: 0 | Loss: 19.023561477661133\n","\tgrad1 & grad2:  1.0 2.0 0.8440313339233398 0.8440313339233398\n","\tgrad1 & grad2:  2.0 4.0 -5.8335466384887695 -11.667093276977539\n","\tgrad1 & grad2:  3.0 6.0 -14.193122863769531 -42.579368591308594\n","Epoch: 1 | Loss: 5.595686912536621\n","\tgrad1 & grad2:  1.0 2.0 2.2957329750061035 2.2957329750061035\n","\tgrad1 & grad2:  2.0 4.0 3.897045135498047 7.794090270996094\n","\tgrad1 & grad2:  3.0 6.0 4.791292190551758 14.373876571655273\n","Epoch: 2 | Loss: 0.6376799941062927\n","\tgrad1 & grad2:  1.0 2.0 1.586777687072754 1.586777687072754\n","\tgrad1 & grad2:  2.0 4.0 -0.7257232666015625 -1.451446533203125\n","\tgrad1 & grad2:  3.0 6.0 -4.061101913452148 -12.183305740356445\n","Epoch: 3 | Loss: 0.45812636613845825\n","\tgrad1 & grad2:  1.0 2.0 1.891737937927246 1.891737937927246\n","\tgrad1 & grad2:  2.0 4.0 1.384765625 2.76953125\n","\tgrad1 & grad2:  3.0 6.0 0.1420269012451172 0.42608070373535156\n","Epoch: 4 | Loss: 0.0005603233585134149\n","\tgrad1 & grad2:  1.0 2.0 1.7216205596923828 1.7216205596923828\n","\tgrad1 & grad2:  2.0 4.0 0.338134765625 0.67626953125\n","\tgrad1 & grad2:  3.0 6.0 -1.7792387008666992 -5.337716102600098\n","Epoch: 5 | Loss: 0.08793584257364273\n","\tgrad1 & grad2:  1.0 2.0 1.7748064994812012 1.7748064994812012\n","\tgrad1 & grad2:  2.0 4.0 0.7733020782470703 1.5466041564941406\n","\tgrad1 & grad2:  3.0 6.0 -0.8288326263427734 -2.4864978790283203\n","Epoch: 6 | Loss: 0.019082320854067802\n","\tgrad1 & grad2:  1.0 2.0 1.7237229347229004 1.7237229347229004\n","\tgrad1 & grad2:  2.0 4.0 0.514434814453125 1.02886962890625\n","\tgrad1 & grad2:  3.0 6.0 -1.2262001037597656 -3.678600311279297\n","Epoch: 7 | Loss: 0.041765742003917694\n","\tgrad1 & grad2:  1.0 2.0 1.7220039367675781 1.7220039367675781\n","\tgrad1 & grad2:  2.0 4.0 0.5820503234863281 1.1641006469726562\n","\tgrad1 & grad2:  3.0 6.0 -0.9922685623168945 -2.9768056869506836\n","Epoch: 8 | Loss: 0.027349913492798805\n","\tgrad1 & grad2:  1.0 2.0 1.6975822448730469 1.6975822448730469\n","\tgrad1 & grad2:  2.0 4.0 0.4974803924560547 0.9949607849121094\n","\tgrad1 & grad2:  3.0 6.0 -1.0552711486816406 -3.165813446044922\n","Epoch: 9 | Loss: 0.030933255329728127\n","Prediction (after training) 4hours 8.59715461730957\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gi-SPOEWYCVu"},"source":["Lecture 05."]},{"cell_type":"code","metadata":{"id":"8JdgJHGQmRNy"},"source":["from torch import nn\n","import torch\n","from torch import tensor\n","\n","x_data = tensor([[1.0],[2.0],[3.0]])\n","y_data = tensor([[2.0],[4.0],[6.0]])\n","\n","class Model(nn.Module):\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.linear = torch.nn.Linear(1,1)\n","\n","  def forward(self, x):\n","    y_pred = self.linear(x)\n","    return y_pred\n","\n","model = Model()\n","\n","criterion = torch.nn.MSELoss(reduction='sum')\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","for epoch in range(500):\n","  y_pred = model(x_data)\n","\n","  loss = criterion(y_pred, y_data)\n","  print(f'Epoch: {epoch} | Loss: {loss.item()}')\n","\n","  optimizer.zero_grad()\n","  loss.backward()\n","  optimizer.step()\n","\n","hour_var = tensor([[4.0]])\n","y_pred = model(hour_var)\n","print('Prediction (after training)', 4, model(hour_var).item())"],"execution_count":null,"outputs":[]}]}
