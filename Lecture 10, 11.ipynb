{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20201117.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM9HfI6o3MchyFWE+gAnK3u"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qy2j48uPatSA"},"source":["Lecture 10."]},{"cell_type":"code","metadata":{"id":"rDkvxKdUaof8"},"source":["import torch\n","import torch.nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    # 인풋 채널 1, 아웃풋 채널 10, 필터 크기 5x5\n","    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","    self.mp = nn.MaxPool2d(2)\n","    self.fc = nn.Linear(320, 10) # 320 -> 10\n","\n","  def forward(self, x):\n","    in_size = x.size(0)\n","    x = F.relu(self.mp(self.conv1(x)))\n","    x = F.relu(self.mp(self.conv2(x)))\n","    x = x.view(in_size, -1)\n","    x = self.fc(x)\n","    return F.log_softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CL02KNd5iXsl"},"source":["Exercise 10-1."]},{"cell_type":"code","metadata":{"id":"upBKb098awEz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605924089580,"user_tz":-540,"elapsed":1005,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"1c2190d6-e940-4d97-ebe0-59d7f079d900"},"source":["import torch\n","from torch import nn\n","import torch.nn\n","import torch.nn.functional as F\n","\n","class Basic(nn.Module):\n","\n","  def __init__(self):\n","    super(Basic, self).__init__()\n","    self.conv1 = nn.Conv2d(3, 32, kernel_size = 3, stride = 1, padding = 1)\n","    self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n","    self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n","    self.conv4 = nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1)\n","    self.mp = nn.MaxPool2d(kernel_size = 2, padding = 2)\n","    self.fc1 = nn.Linear(20736, 256)\n","    self.fc2 = nn.Linear(256, 10)\n","\n","  def forward(self, x):\n","    in_size = x.size(0)\n","    x = F.relu(self.mp(self.conv1(x)))\n","    x = F.relu(self.mp(self.conv2(x)))\n","    x = F.relu(self.mp(self.conv3(x)))\n","    x = F.relu(self.mp(self.conv4(x)))\n","    x = x.view(in_size, -1)\n","    x = self.fc1(x)\n","    x = self.fc2(x)\n","    return F.log_softmax(x)\n","\n","model = Basic()\n","from torchsummary import summary\n","summary(model, (3,128,128))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 128, 128]             896\n","         MaxPool2d-2           [-1, 32, 65, 65]               0\n","            Conv2d-3           [-1, 64, 65, 65]          18,496\n","         MaxPool2d-4           [-1, 64, 33, 33]               0\n","            Conv2d-5          [-1, 128, 33, 33]          73,856\n","         MaxPool2d-6          [-1, 128, 17, 17]               0\n","            Conv2d-7          [-1, 256, 17, 17]         295,168\n","         MaxPool2d-8            [-1, 256, 9, 9]               0\n","            Linear-9                  [-1, 256]       5,308,672\n","           Linear-10                   [-1, 10]           2,570\n","================================================================\n","Total params: 5,699,658\n","Trainable params: 5,699,658\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.19\n","Forward/backward pass size (MB): 9.70\n","Params size (MB): 21.74\n","Estimated Total Size (MB): 31.63\n","----------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NkIJEf2ZhmqQ"},"source":["Exercise 11-1. GoogLeNet"]},{"cell_type":"code","metadata":{"id":"FZxgKONGimIb"},"source":["import torch\n","import torch.nn as nn\n","        \n","class Inception(nn.Module): # padding은 공식 이용해서 계산, stride는 주어짐, 'S'는 'same', 'V'는 'valid'로 간주\n","    \n","    def __init__(self, common_in_ch, o_c1, o_c2a, i_c2, o_c2b, o_c3a, i_c3, o_c3b, o_c4): \n","        super(Inception, self).__init__()\n","        \n","        branch1 = []\n","        branch1 += [nn.Conv2d(in_channels = common_in_ch, out_channels = o_c1, kernel_size = 1, stride = 1, padding = 0),\n","                    nn.ReLU(True)]\n","        \n","        branch2 = []\n","        branch2 += [nn.Conv2d(in_channels = common_in_ch, out_channels = o_c2a, kernel_size = 1, stride = 1, padding = 0),\n","                    nn.ReLU(True),\n","                    nn.Conv2d(in_channels = i_c2, out_channels = o_c2b, kernel_size = 3, stride = 1, padding = 1),\n","                    nn.ReLU(True)]\n","        \n","        branch3 = []\n","        branch3 += [nn.Conv2d(in_channels = common_in_ch, out_channels = o_c3a, kernel_size = 1, stride = 1, padding = 0),\n","                    nn.ReLU(True),\n","                    nn.Conv2d(in_channels = i_c3, out_channels = o_c3b, kernel_size = 5, stride = 1, padding = 2),\n","                    nn.ReLU(True)]\n","        \n","        branch4 = []\n","        branch4 += [nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),\n","                    nn.Conv2d(in_channels = common_in_ch, out_channels = o_c4, kernel_size = 1, padding = 0),\n","                    nn.ReLU(True)]\n","        \n","        self.layer1 = nn.Sequential(*branch1)\n","        self.layer2 = nn.Sequential(*branch2)\n","        self.layer3 = nn.Sequential(*branch3)\n","        self.layer4 = nn.Sequential(*branch4)\n","        \n","    def forward(self, x):\n","        \n","        x1 = self.layer1(x)\n","        x2 = self.layer2(x)\n","        x3 = self.layer3(x)\n","        x4 = self.layer4(x)\n","        \n","        out = torch.cat((x1, x2, x3, x4), dim = 1) # (batchsize, channel, width, height)\n","        \n","        return out\n","\n","class GoogleNet(nn.Module):\n","    \n","    def __init__(self):\n","        super(GoogleNet, self).__init__()\n","        \n","        layer1 = []\n","        layer1 += [nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding = 3), \n","                   nn.ReLU(True),\n","                   nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),\n","                   nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 1, stride = 1, padding = 0), # (V)로 표시되서 valid로 간주\n","                   nn.ReLU(True),\n","                   nn.Conv2d(in_channels = 64, out_channels = 192, kernel_size = 3, stride = 1, padding = 1),\n","                   nn.ReLU(True),\n","                   Inception(192, 64, 96, 96, 128, 16, 16, 32, 32),\n","                   Inception(256, 128, 128, 128, 192, 32, 32, 96, 64),\n","                   nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),\n","                   Inception(480, 192, 96, 96, 208, 16, 16, 48, 64),\n","                   Inception(512, 160, 112, 112, 224, 24, 24, 64, 64)]\n","        \n","        layer2 = []\n","        layer2 += [Inception(512, 128, 128, 128, 256, 24, 24, 64, 64),\n","                   Inception(512, 112, 144, 144, 288, 32, 32, 64, 64),\n","                   Inception(528, 256, 160, 160, 320, 32, 32, 128, 128)]\n","        \n","        layer3 = []\n","        layer3 += [nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),\n","                   Inception(832, 256, 160, 160, 320, 32, 32, 128, 128),\n","                   Inception(832, 384, 192, 192, 384, 48, 48, 128, 128),\n","                   nn.AdaptiveAvgPool2d((7,7)),\n","                   nn.Dropout(0.4)]\n","        \n","        self.layer1 = nn.Sequential(*layer1)\n","        self.layer2 = nn.Sequential(*layer2)\n","        self.layer3 = nn.Sequential(*layer3)\n","        \n","        self.conv = nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 1, stride = 1, padding = 1)\n","        \n","        self.dense1a = nn.Linear(in_features = 65664, out_features = 132096) # 이것들은 모두 keras의 summary를 통해서 알아냈음\n","        self.dense1b = nn.Linear(in_features = 106624, out_features = 132096)\n","        \n","        self.dense2 = nn.Linear(in_features = 132096, out_features = 1049600)\n","        \n","        self.dense3 = nn.Linear(in_features = 1024, out_features = 1000)\n","        \n","        self.avgpool = nn.AdaptiveAvgPool2d((5,5)) # AdaptiveAvgPool2d은 이 형태로만, stride와 padding 안들어감\n","        \n","    def forward(self, x):\n","        \n","        x = self.layer1(x)\n","        \n","        aux0 = self.avgpool(x)\n","        aux0 = self.conv(aux0)\n","        aux0 = self.dense1a(aux0)\n","        aux0 = self.dense2(aux0)\n","        \n","        x = self.layer2(x)\n","        \n","        aux1 = self.avgpool(x)\n","        aux1 = self.conv(aux1)\n","        aux1 = self.dense1b(aux1)\n","        aux1 = self.dense2(aux1)\n","        \n","        x = self.layer3(x)\n","        x = x.view(x.size(0),-1)\n","        x =  self.dense3(x)\n","        \n","        return x, aux0, aux1\n","\n","from torchsummary import summary\n","model = GoogleNet()\n","summary(model, (3, 224, 224))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9TI9OqdFiAKX"},"source":["Exercis 11-2. ResNet"]},{"cell_type":"code","metadata":{"id":"KnlrRzxYSlqS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608432349169,"user_tz":-540,"elapsed":1138,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"6bb984c7-da98-4fd1-8772-75fa1b2d44bf"},"source":["import torch\n","import torch.nn as nn\n","\n","class View(nn.Module):\n","    \n","    def __init__(self, *shape): \n","        super(View, self).__init__() \n","        self.shape = shape\n","        \n","    def forward(self, x):\n","        return x.view(x.shape[0], *self.shape) \n","\n","class Residual_Block(nn.Module):  \n","    \n","    def __init__(self, n_ch): \n","        super(Residual_Block, self).__init__() \n","        layers = []\n","        layers += [nn.BatchNorm2d(num_features=n_ch),\n","                  nn.ReLU(inplace=True), \n","                  nn.Conv2d(in_channels=n_ch, out_channels=n_ch, kernel_size=3, stride=1, padding=1, bias=False),\n","                  nn.BatchNorm2d(num_features=n_ch),\n","                  nn.ReLU(inplace=True),\n","                  nn.Conv2d(in_channels=n_ch, out_channels=n_ch, kernel_size=3, stride=1, padding=1, bias=False)]\n","        self.layers = nn.Sequential(*layers)\n","        \n","    def forward(self,x):\n","        out = self.layers(x)\n","        return x + out\n","    \n","class ResNet(nn.Module):\n","    \n","    def __init__(self):\n","        super(ResNet, self).__init__()\n","        \n","        layers = []\n","        layers += [nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3), \n","                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1), \n","                   Residual_Block(n_ch=64),\n","                   Residual_Block(n_ch=64),\n","                   nn.BatchNorm2d(64), \n","                   nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1),\n","                   Residual_Block(n_ch=256),\n","                   Residual_Block(n_ch=256), \n","                   nn.AdaptiveAvgPool2d((1,1)), \n","                   View(-1), \n","                   nn.Linear(in_features=256, out_features=10)]\n","                      \n","        self.layers = nn.Sequential(*layers)\n","        \n","    def forward(self,x):\n","        return self.layers(x)\n","    \n","if __name__ == '__main__': \n","    model = ResNet()\n","    summary(model, (1,28,28))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 14, 14]           3,200\n","         MaxPool2d-2             [-1, 64, 7, 7]               0\n","       BatchNorm2d-3             [-1, 64, 7, 7]             128\n","              ReLU-4             [-1, 64, 7, 7]               0\n","            Conv2d-5             [-1, 64, 7, 7]          36,864\n","       BatchNorm2d-6             [-1, 64, 7, 7]             128\n","              ReLU-7             [-1, 64, 7, 7]               0\n","            Conv2d-8             [-1, 64, 7, 7]          36,864\n","    Residual_Block-9             [-1, 64, 7, 7]               0\n","      BatchNorm2d-10             [-1, 64, 7, 7]             128\n","             ReLU-11             [-1, 64, 7, 7]               0\n","           Conv2d-12             [-1, 64, 7, 7]          36,864\n","      BatchNorm2d-13             [-1, 64, 7, 7]             128\n","             ReLU-14             [-1, 64, 7, 7]               0\n","           Conv2d-15             [-1, 64, 7, 7]          36,864\n","   Residual_Block-16             [-1, 64, 7, 7]               0\n","      BatchNorm2d-17             [-1, 64, 7, 7]             128\n","           Conv2d-18            [-1, 256, 7, 7]         147,712\n","      BatchNorm2d-19            [-1, 256, 7, 7]             512\n","             ReLU-20            [-1, 256, 7, 7]               0\n","           Conv2d-21            [-1, 256, 7, 7]         589,824\n","      BatchNorm2d-22            [-1, 256, 7, 7]             512\n","             ReLU-23            [-1, 256, 7, 7]               0\n","           Conv2d-24            [-1, 256, 7, 7]         589,824\n","   Residual_Block-25            [-1, 256, 7, 7]               0\n","      BatchNorm2d-26            [-1, 256, 7, 7]             512\n","             ReLU-27            [-1, 256, 7, 7]               0\n","           Conv2d-28            [-1, 256, 7, 7]         589,824\n","      BatchNorm2d-29            [-1, 256, 7, 7]             512\n","             ReLU-30            [-1, 256, 7, 7]               0\n","           Conv2d-31            [-1, 256, 7, 7]         589,824\n","   Residual_Block-32            [-1, 256, 7, 7]               0\n","AdaptiveAvgPool2d-33            [-1, 256, 1, 1]               0\n","             View-34                  [-1, 256]               0\n","           Linear-35                   [-1, 10]           2,570\n","================================================================\n","Total params: 2,662,922\n","Trainable params: 2,662,922\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.92\n","Params size (MB): 10.16\n","Estimated Total Size (MB): 12.08\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z5i0E4bgvXjH"},"source":["Exercise 11-3. DenseNet"]},{"cell_type":"code","metadata":{"id":"lJ4vr4-rpDjH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606023136872,"user_tz":-540,"elapsed":2246,"user":{"displayName":"차정훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6xfHEoVdjEJ4nV7mmInM9vvYue0PAZ2x7u-co=s64","userId":"17408755466079675051"}},"outputId":"3a8b5600-b731-4e32-d4da-5306cfa2597b"},"source":["import torch\n","import torch.nn as nn\n","\n","class Bottleneck(nn.Module):\n","\n","  def __init__(self, in_ch, growth_rate):\n","    super(Bottleneck, self).__init__()\n","    self.bn1 = nn.BatchNorm2d(in_ch)\n","    self.conv1 = nn.Conv2d(in_ch, growth_rate*4, kernel_size=1, bias=False)\n","    self.bn2 = nn.BatchNorm2d(growth_rate*4)\n","    self.conv2 = nn.Conv2d(growth_rate*4, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n","    self.relu = nn.ReLU(True)\n","\n","  def forward(self, x):\n","    x1 = self.bn1(x)\n","    x1 = self.relu(x1)\n","    x1 = self.conv1(x1)\n","    x1 = self.bn2(x1)\n","    x1 = self.relu(x1)\n","    x1 = self.conv2(x1)\n","    x = torch.cat((x, x1), dim=1)\n","    return x\n","\n","class DenseBlock(nn.Module):\n","\n","  def __init__(self, n_bottleneck, in_ch, growth_rate):\n","    super(DenseBlock, self).__init__()\n","\n","    for i in range(n_bottleneck):\n","      # i번째 DenseBlock을 생성하고 거기에 Bottleneck의 속성을 차례대로 부여한다.\n","      setattr(self, 'DenseBlock_{}'.format(i), Bottleneck(in_ch+i*growth_rate, growth_rate))\n","\n","    # forward 함수에서 n_bottleneck를 쓰기 위함\n","    self.n_bottleneck = n_bottleneck\n","\n","  def forward(self, x):\n","\n","    for i in range(self.n_bottleneck):\n","      # setattr을 통해 만들어진 DenseBlock들을 찾아오고, 평소처럼 레이어 x에 연결된다.\n","      x = getattr(self, 'DenseBlock_{}'.format(i))(x)\n","    return x\n","\n","class Transition(nn.Module):\n","\n","  def __init__(self, in_ch):\n","    super(Transition, self).__init__()\n","    num_ch = int(in_ch*0.5) # 논문에서 제시한 세타 값 = 0.5\n","    self.bn = nn.BatchNorm2d(in_ch)\n","    self.relu = nn.ReLU(True)\n","    self.conv = nn.Conv2d(in_ch, num_ch, kernel_size=3, padding=1, bias=False)\n","    self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","  def forward(self,x):\n","    x = self.bn(x)\n","    x = self.relu(x)\n","    x = self.conv(x)\n","    x = self.avgpool(x)\n","    return x\n","\n","class DenseNet(nn.Module): # DenseNet-121\n","\n","  def __init__(self):\n","    super(DenseNet, self).__init__()\n","    self.conv = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","    self.bn1 = nn.BatchNorm2d(64)\n","    self.relu = nn.ReLU(True)\n","    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","    self.dense1 = DenseBlock(6, 64, 32) # growth rate는 논문에서 항상 32로 고정됨\n","    self.trans1 = Transition(256) # 최종 in_ch+i*growth_rate -> 64+6*32 = 256\n","    self.dense2 = DenseBlock(12, 128, 32) # Transition의 conv 거치면 채널 수 절반으로(세타=0.5 영향). 256/2 = 128\n","    self.trans2 = Transition(512) # 128+12*32 = 512\n","    self.dense3 = DenseBlock(24, 256, 32) # 512/2 = 256\n","    self.trans3 = Transition(1024) # 256+24*32 = 1024\n","    self.dense4 = DenseBlock(16, 512, 32)\n","    self.avgpool = nn.AdaptiveAvgPool2d((7,7))\n","    self.linear = nn.Linear(50176, 10)\n","\n","  def forward(self, x):\n","    in_size = x.size(0)\n","    x = self.conv(x)\n","    x = self.bn1(x)\n","    x = self.relu(x)\n","    x = self.maxpool(x)\n","    x = self.dense1(x)\n","    x = self.trans1(x)\n","    x = self.dense2(x)\n","    x = self.trans2(x)\n","    x = self.dense3(x)\n","    x = self.trans3(x)\n","    x = self.dense4(x)\n","    x = self.avgpool(x)\n","    x = x.view(in_size, -1)\n","    x = self.linear(x)\n","\n","model = DenseNet()\n","from torchsummary import summary\n","summary(model, (3,224,224))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","       BatchNorm2d-5           [-1, 64, 56, 56]             128\n","              ReLU-6           [-1, 64, 56, 56]               0\n","            Conv2d-7          [-1, 128, 56, 56]           8,192\n","       BatchNorm2d-8          [-1, 128, 56, 56]             256\n","              ReLU-9          [-1, 128, 56, 56]               0\n","           Conv2d-10           [-1, 32, 56, 56]          36,864\n","       Bottleneck-11           [-1, 96, 56, 56]               0\n","      BatchNorm2d-12           [-1, 96, 56, 56]             192\n","             ReLU-13           [-1, 96, 56, 56]               0\n","           Conv2d-14          [-1, 128, 56, 56]          12,288\n","      BatchNorm2d-15          [-1, 128, 56, 56]             256\n","             ReLU-16          [-1, 128, 56, 56]               0\n","           Conv2d-17           [-1, 32, 56, 56]          36,864\n","       Bottleneck-18          [-1, 128, 56, 56]               0\n","      BatchNorm2d-19          [-1, 128, 56, 56]             256\n","             ReLU-20          [-1, 128, 56, 56]               0\n","           Conv2d-21          [-1, 128, 56, 56]          16,384\n","      BatchNorm2d-22          [-1, 128, 56, 56]             256\n","             ReLU-23          [-1, 128, 56, 56]               0\n","           Conv2d-24           [-1, 32, 56, 56]          36,864\n","       Bottleneck-25          [-1, 160, 56, 56]               0\n","      BatchNorm2d-26          [-1, 160, 56, 56]             320\n","             ReLU-27          [-1, 160, 56, 56]               0\n","           Conv2d-28          [-1, 128, 56, 56]          20,480\n","      BatchNorm2d-29          [-1, 128, 56, 56]             256\n","             ReLU-30          [-1, 128, 56, 56]               0\n","           Conv2d-31           [-1, 32, 56, 56]          36,864\n","       Bottleneck-32          [-1, 192, 56, 56]               0\n","      BatchNorm2d-33          [-1, 192, 56, 56]             384\n","             ReLU-34          [-1, 192, 56, 56]               0\n","           Conv2d-35          [-1, 128, 56, 56]          24,576\n","      BatchNorm2d-36          [-1, 128, 56, 56]             256\n","             ReLU-37          [-1, 128, 56, 56]               0\n","           Conv2d-38           [-1, 32, 56, 56]          36,864\n","       Bottleneck-39          [-1, 224, 56, 56]               0\n","      BatchNorm2d-40          [-1, 224, 56, 56]             448\n","             ReLU-41          [-1, 224, 56, 56]               0\n","           Conv2d-42          [-1, 128, 56, 56]          28,672\n","      BatchNorm2d-43          [-1, 128, 56, 56]             256\n","             ReLU-44          [-1, 128, 56, 56]               0\n","           Conv2d-45           [-1, 32, 56, 56]          36,864\n","       Bottleneck-46          [-1, 256, 56, 56]               0\n","       DenseBlock-47          [-1, 256, 56, 56]               0\n","      BatchNorm2d-48          [-1, 256, 56, 56]             512\n","             ReLU-49          [-1, 256, 56, 56]               0\n","           Conv2d-50          [-1, 128, 56, 56]         294,912\n","        AvgPool2d-51          [-1, 128, 28, 28]               0\n","       Transition-52          [-1, 128, 28, 28]               0\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","           Conv2d-55          [-1, 128, 28, 28]          16,384\n","      BatchNorm2d-56          [-1, 128, 28, 28]             256\n","             ReLU-57          [-1, 128, 28, 28]               0\n","           Conv2d-58           [-1, 32, 28, 28]          36,864\n","       Bottleneck-59          [-1, 160, 28, 28]               0\n","      BatchNorm2d-60          [-1, 160, 28, 28]             320\n","             ReLU-61          [-1, 160, 28, 28]               0\n","           Conv2d-62          [-1, 128, 28, 28]          20,480\n","      BatchNorm2d-63          [-1, 128, 28, 28]             256\n","             ReLU-64          [-1, 128, 28, 28]               0\n","           Conv2d-65           [-1, 32, 28, 28]          36,864\n","       Bottleneck-66          [-1, 192, 28, 28]               0\n","      BatchNorm2d-67          [-1, 192, 28, 28]             384\n","             ReLU-68          [-1, 192, 28, 28]               0\n","           Conv2d-69          [-1, 128, 28, 28]          24,576\n","      BatchNorm2d-70          [-1, 128, 28, 28]             256\n","             ReLU-71          [-1, 128, 28, 28]               0\n","           Conv2d-72           [-1, 32, 28, 28]          36,864\n","       Bottleneck-73          [-1, 224, 28, 28]               0\n","      BatchNorm2d-74          [-1, 224, 28, 28]             448\n","             ReLU-75          [-1, 224, 28, 28]               0\n","           Conv2d-76          [-1, 128, 28, 28]          28,672\n","      BatchNorm2d-77          [-1, 128, 28, 28]             256\n","             ReLU-78          [-1, 128, 28, 28]               0\n","           Conv2d-79           [-1, 32, 28, 28]          36,864\n","       Bottleneck-80          [-1, 256, 28, 28]               0\n","      BatchNorm2d-81          [-1, 256, 28, 28]             512\n","             ReLU-82          [-1, 256, 28, 28]               0\n","           Conv2d-83          [-1, 128, 28, 28]          32,768\n","      BatchNorm2d-84          [-1, 128, 28, 28]             256\n","             ReLU-85          [-1, 128, 28, 28]               0\n","           Conv2d-86           [-1, 32, 28, 28]          36,864\n","       Bottleneck-87          [-1, 288, 28, 28]               0\n","      BatchNorm2d-88          [-1, 288, 28, 28]             576\n","             ReLU-89          [-1, 288, 28, 28]               0\n","           Conv2d-90          [-1, 128, 28, 28]          36,864\n","      BatchNorm2d-91          [-1, 128, 28, 28]             256\n","             ReLU-92          [-1, 128, 28, 28]               0\n","           Conv2d-93           [-1, 32, 28, 28]          36,864\n","       Bottleneck-94          [-1, 320, 28, 28]               0\n","      BatchNorm2d-95          [-1, 320, 28, 28]             640\n","             ReLU-96          [-1, 320, 28, 28]               0\n","           Conv2d-97          [-1, 128, 28, 28]          40,960\n","      BatchNorm2d-98          [-1, 128, 28, 28]             256\n","             ReLU-99          [-1, 128, 28, 28]               0\n","          Conv2d-100           [-1, 32, 28, 28]          36,864\n","      Bottleneck-101          [-1, 352, 28, 28]               0\n","     BatchNorm2d-102          [-1, 352, 28, 28]             704\n","            ReLU-103          [-1, 352, 28, 28]               0\n","          Conv2d-104          [-1, 128, 28, 28]          45,056\n","     BatchNorm2d-105          [-1, 128, 28, 28]             256\n","            ReLU-106          [-1, 128, 28, 28]               0\n","          Conv2d-107           [-1, 32, 28, 28]          36,864\n","      Bottleneck-108          [-1, 384, 28, 28]               0\n","     BatchNorm2d-109          [-1, 384, 28, 28]             768\n","            ReLU-110          [-1, 384, 28, 28]               0\n","          Conv2d-111          [-1, 128, 28, 28]          49,152\n","     BatchNorm2d-112          [-1, 128, 28, 28]             256\n","            ReLU-113          [-1, 128, 28, 28]               0\n","          Conv2d-114           [-1, 32, 28, 28]          36,864\n","      Bottleneck-115          [-1, 416, 28, 28]               0\n","     BatchNorm2d-116          [-1, 416, 28, 28]             832\n","            ReLU-117          [-1, 416, 28, 28]               0\n","          Conv2d-118          [-1, 128, 28, 28]          53,248\n","     BatchNorm2d-119          [-1, 128, 28, 28]             256\n","            ReLU-120          [-1, 128, 28, 28]               0\n","          Conv2d-121           [-1, 32, 28, 28]          36,864\n","      Bottleneck-122          [-1, 448, 28, 28]               0\n","     BatchNorm2d-123          [-1, 448, 28, 28]             896\n","            ReLU-124          [-1, 448, 28, 28]               0\n","          Conv2d-125          [-1, 128, 28, 28]          57,344\n","     BatchNorm2d-126          [-1, 128, 28, 28]             256\n","            ReLU-127          [-1, 128, 28, 28]               0\n","          Conv2d-128           [-1, 32, 28, 28]          36,864\n","      Bottleneck-129          [-1, 480, 28, 28]               0\n","     BatchNorm2d-130          [-1, 480, 28, 28]             960\n","            ReLU-131          [-1, 480, 28, 28]               0\n","          Conv2d-132          [-1, 128, 28, 28]          61,440\n","     BatchNorm2d-133          [-1, 128, 28, 28]             256\n","            ReLU-134          [-1, 128, 28, 28]               0\n","          Conv2d-135           [-1, 32, 28, 28]          36,864\n","      Bottleneck-136          [-1, 512, 28, 28]               0\n","      DenseBlock-137          [-1, 512, 28, 28]               0\n","     BatchNorm2d-138          [-1, 512, 28, 28]           1,024\n","            ReLU-139          [-1, 512, 28, 28]               0\n","          Conv2d-140          [-1, 256, 28, 28]       1,179,648\n","       AvgPool2d-141          [-1, 256, 14, 14]               0\n","      Transition-142          [-1, 256, 14, 14]               0\n","     BatchNorm2d-143          [-1, 256, 14, 14]             512\n","            ReLU-144          [-1, 256, 14, 14]               0\n","          Conv2d-145          [-1, 128, 14, 14]          32,768\n","     BatchNorm2d-146          [-1, 128, 14, 14]             256\n","            ReLU-147          [-1, 128, 14, 14]               0\n","          Conv2d-148           [-1, 32, 14, 14]          36,864\n","      Bottleneck-149          [-1, 288, 14, 14]               0\n","     BatchNorm2d-150          [-1, 288, 14, 14]             576\n","            ReLU-151          [-1, 288, 14, 14]               0\n","          Conv2d-152          [-1, 128, 14, 14]          36,864\n","     BatchNorm2d-153          [-1, 128, 14, 14]             256\n","            ReLU-154          [-1, 128, 14, 14]               0\n","          Conv2d-155           [-1, 32, 14, 14]          36,864\n","      Bottleneck-156          [-1, 320, 14, 14]               0\n","     BatchNorm2d-157          [-1, 320, 14, 14]             640\n","            ReLU-158          [-1, 320, 14, 14]               0\n","          Conv2d-159          [-1, 128, 14, 14]          40,960\n","     BatchNorm2d-160          [-1, 128, 14, 14]             256\n","            ReLU-161          [-1, 128, 14, 14]               0\n","          Conv2d-162           [-1, 32, 14, 14]          36,864\n","      Bottleneck-163          [-1, 352, 14, 14]               0\n","     BatchNorm2d-164          [-1, 352, 14, 14]             704\n","            ReLU-165          [-1, 352, 14, 14]               0\n","          Conv2d-166          [-1, 128, 14, 14]          45,056\n","     BatchNorm2d-167          [-1, 128, 14, 14]             256\n","            ReLU-168          [-1, 128, 14, 14]               0\n","          Conv2d-169           [-1, 32, 14, 14]          36,864\n","      Bottleneck-170          [-1, 384, 14, 14]               0\n","     BatchNorm2d-171          [-1, 384, 14, 14]             768\n","            ReLU-172          [-1, 384, 14, 14]               0\n","          Conv2d-173          [-1, 128, 14, 14]          49,152\n","     BatchNorm2d-174          [-1, 128, 14, 14]             256\n","            ReLU-175          [-1, 128, 14, 14]               0\n","          Conv2d-176           [-1, 32, 14, 14]          36,864\n","      Bottleneck-177          [-1, 416, 14, 14]               0\n","     BatchNorm2d-178          [-1, 416, 14, 14]             832\n","            ReLU-179          [-1, 416, 14, 14]               0\n","          Conv2d-180          [-1, 128, 14, 14]          53,248\n","     BatchNorm2d-181          [-1, 128, 14, 14]             256\n","            ReLU-182          [-1, 128, 14, 14]               0\n","          Conv2d-183           [-1, 32, 14, 14]          36,864\n","      Bottleneck-184          [-1, 448, 14, 14]               0\n","     BatchNorm2d-185          [-1, 448, 14, 14]             896\n","            ReLU-186          [-1, 448, 14, 14]               0\n","          Conv2d-187          [-1, 128, 14, 14]          57,344\n","     BatchNorm2d-188          [-1, 128, 14, 14]             256\n","            ReLU-189          [-1, 128, 14, 14]               0\n","          Conv2d-190           [-1, 32, 14, 14]          36,864\n","      Bottleneck-191          [-1, 480, 14, 14]               0\n","     BatchNorm2d-192          [-1, 480, 14, 14]             960\n","            ReLU-193          [-1, 480, 14, 14]               0\n","          Conv2d-194          [-1, 128, 14, 14]          61,440\n","     BatchNorm2d-195          [-1, 128, 14, 14]             256\n","            ReLU-196          [-1, 128, 14, 14]               0\n","          Conv2d-197           [-1, 32, 14, 14]          36,864\n","      Bottleneck-198          [-1, 512, 14, 14]               0\n","     BatchNorm2d-199          [-1, 512, 14, 14]           1,024\n","            ReLU-200          [-1, 512, 14, 14]               0\n","          Conv2d-201          [-1, 128, 14, 14]          65,536\n","     BatchNorm2d-202          [-1, 128, 14, 14]             256\n","            ReLU-203          [-1, 128, 14, 14]               0\n","          Conv2d-204           [-1, 32, 14, 14]          36,864\n","      Bottleneck-205          [-1, 544, 14, 14]               0\n","     BatchNorm2d-206          [-1, 544, 14, 14]           1,088\n","            ReLU-207          [-1, 544, 14, 14]               0\n","          Conv2d-208          [-1, 128, 14, 14]          69,632\n","     BatchNorm2d-209          [-1, 128, 14, 14]             256\n","            ReLU-210          [-1, 128, 14, 14]               0\n","          Conv2d-211           [-1, 32, 14, 14]          36,864\n","      Bottleneck-212          [-1, 576, 14, 14]               0\n","     BatchNorm2d-213          [-1, 576, 14, 14]           1,152\n","            ReLU-214          [-1, 576, 14, 14]               0\n","          Conv2d-215          [-1, 128, 14, 14]          73,728\n","     BatchNorm2d-216          [-1, 128, 14, 14]             256\n","            ReLU-217          [-1, 128, 14, 14]               0\n","          Conv2d-218           [-1, 32, 14, 14]          36,864\n","      Bottleneck-219          [-1, 608, 14, 14]               0\n","     BatchNorm2d-220          [-1, 608, 14, 14]           1,216\n","            ReLU-221          [-1, 608, 14, 14]               0\n","          Conv2d-222          [-1, 128, 14, 14]          77,824\n","     BatchNorm2d-223          [-1, 128, 14, 14]             256\n","            ReLU-224          [-1, 128, 14, 14]               0\n","          Conv2d-225           [-1, 32, 14, 14]          36,864\n","      Bottleneck-226          [-1, 640, 14, 14]               0\n","     BatchNorm2d-227          [-1, 640, 14, 14]           1,280\n","            ReLU-228          [-1, 640, 14, 14]               0\n","          Conv2d-229          [-1, 128, 14, 14]          81,920\n","     BatchNorm2d-230          [-1, 128, 14, 14]             256\n","            ReLU-231          [-1, 128, 14, 14]               0\n","          Conv2d-232           [-1, 32, 14, 14]          36,864\n","      Bottleneck-233          [-1, 672, 14, 14]               0\n","     BatchNorm2d-234          [-1, 672, 14, 14]           1,344\n","            ReLU-235          [-1, 672, 14, 14]               0\n","          Conv2d-236          [-1, 128, 14, 14]          86,016\n","     BatchNorm2d-237          [-1, 128, 14, 14]             256\n","            ReLU-238          [-1, 128, 14, 14]               0\n","          Conv2d-239           [-1, 32, 14, 14]          36,864\n","      Bottleneck-240          [-1, 704, 14, 14]               0\n","     BatchNorm2d-241          [-1, 704, 14, 14]           1,408\n","            ReLU-242          [-1, 704, 14, 14]               0\n","          Conv2d-243          [-1, 128, 14, 14]          90,112\n","     BatchNorm2d-244          [-1, 128, 14, 14]             256\n","            ReLU-245          [-1, 128, 14, 14]               0\n","          Conv2d-246           [-1, 32, 14, 14]          36,864\n","      Bottleneck-247          [-1, 736, 14, 14]               0\n","     BatchNorm2d-248          [-1, 736, 14, 14]           1,472\n","            ReLU-249          [-1, 736, 14, 14]               0\n","          Conv2d-250          [-1, 128, 14, 14]          94,208\n","     BatchNorm2d-251          [-1, 128, 14, 14]             256\n","            ReLU-252          [-1, 128, 14, 14]               0\n","          Conv2d-253           [-1, 32, 14, 14]          36,864\n","      Bottleneck-254          [-1, 768, 14, 14]               0\n","     BatchNorm2d-255          [-1, 768, 14, 14]           1,536\n","            ReLU-256          [-1, 768, 14, 14]               0\n","          Conv2d-257          [-1, 128, 14, 14]          98,304\n","     BatchNorm2d-258          [-1, 128, 14, 14]             256\n","            ReLU-259          [-1, 128, 14, 14]               0\n","          Conv2d-260           [-1, 32, 14, 14]          36,864\n","      Bottleneck-261          [-1, 800, 14, 14]               0\n","     BatchNorm2d-262          [-1, 800, 14, 14]           1,600\n","            ReLU-263          [-1, 800, 14, 14]               0\n","          Conv2d-264          [-1, 128, 14, 14]         102,400\n","     BatchNorm2d-265          [-1, 128, 14, 14]             256\n","            ReLU-266          [-1, 128, 14, 14]               0\n","          Conv2d-267           [-1, 32, 14, 14]          36,864\n","      Bottleneck-268          [-1, 832, 14, 14]               0\n","     BatchNorm2d-269          [-1, 832, 14, 14]           1,664\n","            ReLU-270          [-1, 832, 14, 14]               0\n","          Conv2d-271          [-1, 128, 14, 14]         106,496\n","     BatchNorm2d-272          [-1, 128, 14, 14]             256\n","            ReLU-273          [-1, 128, 14, 14]               0\n","          Conv2d-274           [-1, 32, 14, 14]          36,864\n","      Bottleneck-275          [-1, 864, 14, 14]               0\n","     BatchNorm2d-276          [-1, 864, 14, 14]           1,728\n","            ReLU-277          [-1, 864, 14, 14]               0\n","          Conv2d-278          [-1, 128, 14, 14]         110,592\n","     BatchNorm2d-279          [-1, 128, 14, 14]             256\n","            ReLU-280          [-1, 128, 14, 14]               0\n","          Conv2d-281           [-1, 32, 14, 14]          36,864\n","      Bottleneck-282          [-1, 896, 14, 14]               0\n","     BatchNorm2d-283          [-1, 896, 14, 14]           1,792\n","            ReLU-284          [-1, 896, 14, 14]               0\n","          Conv2d-285          [-1, 128, 14, 14]         114,688\n","     BatchNorm2d-286          [-1, 128, 14, 14]             256\n","            ReLU-287          [-1, 128, 14, 14]               0\n","          Conv2d-288           [-1, 32, 14, 14]          36,864\n","      Bottleneck-289          [-1, 928, 14, 14]               0\n","     BatchNorm2d-290          [-1, 928, 14, 14]           1,856\n","            ReLU-291          [-1, 928, 14, 14]               0\n","          Conv2d-292          [-1, 128, 14, 14]         118,784\n","     BatchNorm2d-293          [-1, 128, 14, 14]             256\n","            ReLU-294          [-1, 128, 14, 14]               0\n","          Conv2d-295           [-1, 32, 14, 14]          36,864\n","      Bottleneck-296          [-1, 960, 14, 14]               0\n","     BatchNorm2d-297          [-1, 960, 14, 14]           1,920\n","            ReLU-298          [-1, 960, 14, 14]               0\n","          Conv2d-299          [-1, 128, 14, 14]         122,880\n","     BatchNorm2d-300          [-1, 128, 14, 14]             256\n","            ReLU-301          [-1, 128, 14, 14]               0\n","          Conv2d-302           [-1, 32, 14, 14]          36,864\n","      Bottleneck-303          [-1, 992, 14, 14]               0\n","     BatchNorm2d-304          [-1, 992, 14, 14]           1,984\n","            ReLU-305          [-1, 992, 14, 14]               0\n","          Conv2d-306          [-1, 128, 14, 14]         126,976\n","     BatchNorm2d-307          [-1, 128, 14, 14]             256\n","            ReLU-308          [-1, 128, 14, 14]               0\n","          Conv2d-309           [-1, 32, 14, 14]          36,864\n","      Bottleneck-310         [-1, 1024, 14, 14]               0\n","      DenseBlock-311         [-1, 1024, 14, 14]               0\n","     BatchNorm2d-312         [-1, 1024, 14, 14]           2,048\n","            ReLU-313         [-1, 1024, 14, 14]               0\n","          Conv2d-314          [-1, 512, 14, 14]       4,718,592\n","       AvgPool2d-315            [-1, 512, 7, 7]               0\n","      Transition-316            [-1, 512, 7, 7]               0\n","     BatchNorm2d-317            [-1, 512, 7, 7]           1,024\n","            ReLU-318            [-1, 512, 7, 7]               0\n","          Conv2d-319            [-1, 128, 7, 7]          65,536\n","     BatchNorm2d-320            [-1, 128, 7, 7]             256\n","            ReLU-321            [-1, 128, 7, 7]               0\n","          Conv2d-322             [-1, 32, 7, 7]          36,864\n","      Bottleneck-323            [-1, 544, 7, 7]               0\n","     BatchNorm2d-324            [-1, 544, 7, 7]           1,088\n","            ReLU-325            [-1, 544, 7, 7]               0\n","          Conv2d-326            [-1, 128, 7, 7]          69,632\n","     BatchNorm2d-327            [-1, 128, 7, 7]             256\n","            ReLU-328            [-1, 128, 7, 7]               0\n","          Conv2d-329             [-1, 32, 7, 7]          36,864\n","      Bottleneck-330            [-1, 576, 7, 7]               0\n","     BatchNorm2d-331            [-1, 576, 7, 7]           1,152\n","            ReLU-332            [-1, 576, 7, 7]               0\n","          Conv2d-333            [-1, 128, 7, 7]          73,728\n","     BatchNorm2d-334            [-1, 128, 7, 7]             256\n","            ReLU-335            [-1, 128, 7, 7]               0\n","          Conv2d-336             [-1, 32, 7, 7]          36,864\n","      Bottleneck-337            [-1, 608, 7, 7]               0\n","     BatchNorm2d-338            [-1, 608, 7, 7]           1,216\n","            ReLU-339            [-1, 608, 7, 7]               0\n","          Conv2d-340            [-1, 128, 7, 7]          77,824\n","     BatchNorm2d-341            [-1, 128, 7, 7]             256\n","            ReLU-342            [-1, 128, 7, 7]               0\n","          Conv2d-343             [-1, 32, 7, 7]          36,864\n","      Bottleneck-344            [-1, 640, 7, 7]               0\n","     BatchNorm2d-345            [-1, 640, 7, 7]           1,280\n","            ReLU-346            [-1, 640, 7, 7]               0\n","          Conv2d-347            [-1, 128, 7, 7]          81,920\n","     BatchNorm2d-348            [-1, 128, 7, 7]             256\n","            ReLU-349            [-1, 128, 7, 7]               0\n","          Conv2d-350             [-1, 32, 7, 7]          36,864\n","      Bottleneck-351            [-1, 672, 7, 7]               0\n","     BatchNorm2d-352            [-1, 672, 7, 7]           1,344\n","            ReLU-353            [-1, 672, 7, 7]               0\n","          Conv2d-354            [-1, 128, 7, 7]          86,016\n","     BatchNorm2d-355            [-1, 128, 7, 7]             256\n","            ReLU-356            [-1, 128, 7, 7]               0\n","          Conv2d-357             [-1, 32, 7, 7]          36,864\n","      Bottleneck-358            [-1, 704, 7, 7]               0\n","     BatchNorm2d-359            [-1, 704, 7, 7]           1,408\n","            ReLU-360            [-1, 704, 7, 7]               0\n","          Conv2d-361            [-1, 128, 7, 7]          90,112\n","     BatchNorm2d-362            [-1, 128, 7, 7]             256\n","            ReLU-363            [-1, 128, 7, 7]               0\n","          Conv2d-364             [-1, 32, 7, 7]          36,864\n","      Bottleneck-365            [-1, 736, 7, 7]               0\n","     BatchNorm2d-366            [-1, 736, 7, 7]           1,472\n","            ReLU-367            [-1, 736, 7, 7]               0\n","          Conv2d-368            [-1, 128, 7, 7]          94,208\n","     BatchNorm2d-369            [-1, 128, 7, 7]             256\n","            ReLU-370            [-1, 128, 7, 7]               0\n","          Conv2d-371             [-1, 32, 7, 7]          36,864\n","      Bottleneck-372            [-1, 768, 7, 7]               0\n","     BatchNorm2d-373            [-1, 768, 7, 7]           1,536\n","            ReLU-374            [-1, 768, 7, 7]               0\n","          Conv2d-375            [-1, 128, 7, 7]          98,304\n","     BatchNorm2d-376            [-1, 128, 7, 7]             256\n","            ReLU-377            [-1, 128, 7, 7]               0\n","          Conv2d-378             [-1, 32, 7, 7]          36,864\n","      Bottleneck-379            [-1, 800, 7, 7]               0\n","     BatchNorm2d-380            [-1, 800, 7, 7]           1,600\n","            ReLU-381            [-1, 800, 7, 7]               0\n","          Conv2d-382            [-1, 128, 7, 7]         102,400\n","     BatchNorm2d-383            [-1, 128, 7, 7]             256\n","            ReLU-384            [-1, 128, 7, 7]               0\n","          Conv2d-385             [-1, 32, 7, 7]          36,864\n","      Bottleneck-386            [-1, 832, 7, 7]               0\n","     BatchNorm2d-387            [-1, 832, 7, 7]           1,664\n","            ReLU-388            [-1, 832, 7, 7]               0\n","          Conv2d-389            [-1, 128, 7, 7]         106,496\n","     BatchNorm2d-390            [-1, 128, 7, 7]             256\n","            ReLU-391            [-1, 128, 7, 7]               0\n","          Conv2d-392             [-1, 32, 7, 7]          36,864\n","      Bottleneck-393            [-1, 864, 7, 7]               0\n","     BatchNorm2d-394            [-1, 864, 7, 7]           1,728\n","            ReLU-395            [-1, 864, 7, 7]               0\n","          Conv2d-396            [-1, 128, 7, 7]         110,592\n","     BatchNorm2d-397            [-1, 128, 7, 7]             256\n","            ReLU-398            [-1, 128, 7, 7]               0\n","          Conv2d-399             [-1, 32, 7, 7]          36,864\n","      Bottleneck-400            [-1, 896, 7, 7]               0\n","     BatchNorm2d-401            [-1, 896, 7, 7]           1,792\n","            ReLU-402            [-1, 896, 7, 7]               0\n","          Conv2d-403            [-1, 128, 7, 7]         114,688\n","     BatchNorm2d-404            [-1, 128, 7, 7]             256\n","            ReLU-405            [-1, 128, 7, 7]               0\n","          Conv2d-406             [-1, 32, 7, 7]          36,864\n","      Bottleneck-407            [-1, 928, 7, 7]               0\n","     BatchNorm2d-408            [-1, 928, 7, 7]           1,856\n","            ReLU-409            [-1, 928, 7, 7]               0\n","          Conv2d-410            [-1, 128, 7, 7]         118,784\n","     BatchNorm2d-411            [-1, 128, 7, 7]             256\n","            ReLU-412            [-1, 128, 7, 7]               0\n","          Conv2d-413             [-1, 32, 7, 7]          36,864\n","      Bottleneck-414            [-1, 960, 7, 7]               0\n","     BatchNorm2d-415            [-1, 960, 7, 7]           1,920\n","            ReLU-416            [-1, 960, 7, 7]               0\n","          Conv2d-417            [-1, 128, 7, 7]         122,880\n","     BatchNorm2d-418            [-1, 128, 7, 7]             256\n","            ReLU-419            [-1, 128, 7, 7]               0\n","          Conv2d-420             [-1, 32, 7, 7]          36,864\n","      Bottleneck-421            [-1, 992, 7, 7]               0\n","     BatchNorm2d-422            [-1, 992, 7, 7]           1,984\n","            ReLU-423            [-1, 992, 7, 7]               0\n","          Conv2d-424            [-1, 128, 7, 7]         126,976\n","     BatchNorm2d-425            [-1, 128, 7, 7]             256\n","            ReLU-426            [-1, 128, 7, 7]               0\n","          Conv2d-427             [-1, 32, 7, 7]          36,864\n","      Bottleneck-428           [-1, 1024, 7, 7]               0\n","      DenseBlock-429           [-1, 1024, 7, 7]               0\n","AdaptiveAvgPool2d-430           [-1, 1024, 7, 7]               0\n","          Linear-431                   [-1, 10]         501,770\n","================================================================\n","Total params: 12,958,602\n","Trainable params: 12,958,602\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 384.25\n","Params size (MB): 49.43\n","Estimated Total Size (MB): 434.26\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jSFcDuo9Vof0"},"source":[""],"execution_count":null,"outputs":[]}]}